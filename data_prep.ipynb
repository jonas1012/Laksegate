{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c66e505",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Preparation of multiple datasets\n",
    "## Air quality (Daily)\n",
    "\n",
    "The full dataset is only daily data, and is aggreagting data where multiple time of the day exists by the mean. \n",
    "\n",
    "Importing the notebooks for preparing the data. The data preparation should combine different csv files into a single combined csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf4f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "csv_directory_NO2 = './Air Quality dataset/NO2'  # Update this path to your folder containing CSV files\n",
    "csv_directory_NOx = './Air Quality dataset/NOx'  # Update this path to your folder containing CSV files\n",
    "output_file = './Air Quality dataset/combined_air_full_wide.csv'  # Output file path\n",
    "\n",
    "# Initialize an empty DataFrame for combining data\n",
    "combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f913b",
   "metadata": {},
   "source": [
    "The following code should create a date variable, that stretches from the earliest recorded date to the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6d475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame with Date Range:\n",
      "        Date\n",
      "0 2005-01-01\n",
      "1 2005-01-02\n",
      "2 2005-01-03\n",
      "3 2005-01-04\n",
      "4 2005-01-05\n",
      "           Date\n",
      "6934 2023-12-27\n",
      "6935 2023-12-28\n",
      "6936 2023-12-29\n",
      "6937 2023-12-30\n",
      "6938 2023-12-31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List to store all dates\n",
    "all_dates = []\n",
    "\n",
    "# Iterate through all CSV files in both directories\n",
    "for file in os.listdir(csv_directory_NO2):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NO2, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            all_dates.extend(pd.to_datetime(df['DatoMaerke']).tolist())\n",
    "for file in os.listdir(csv_directory_NOx):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NOx, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            all_dates.extend(pd.to_datetime(df['DatoMaerke']).tolist())\n",
    "\n",
    "# Create a date range from the earliest to the latest date\n",
    "if all_dates:\n",
    "    min_date = min(all_dates)\n",
    "    max_date = max(all_dates)\n",
    "    date_range = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "    # Add the date range to the combined_df\n",
    "    combined_df['Date'] = date_range\n",
    "\n",
    "# Print the first few rows of the combined DataFrame\n",
    "print(\"Combined DataFrame with Date Range:\")\n",
    "print(combined_df.head())\n",
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58a00d",
   "metadata": {},
   "source": [
    "Now, the different NO2 values should be added. The NO2 values are stored in the column called 'NO2 ppb', and each new column should have the name of the file that it was extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0156b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Combined DataFrame with NO2 Values:\n",
      "        Date  NO2_HCAB  NO2_JAGT  NO2_HCØ  NO2_HVID\n",
      "0 2005-01-01       NaN       NaN      NaN       NaN\n",
      "1 2005-01-02       NaN       NaN      NaN       NaN\n",
      "2 2005-01-03       NaN       NaN      NaN       NaN\n",
      "3 2005-01-04       NaN       NaN      NaN       NaN\n",
      "4 2005-01-05       NaN       NaN      NaN       NaN\n",
      "           Date   NO2_HCAB  NO2_JAGT   NO2_HCØ  NO2_HVID\n",
      "6934 2023-12-27  11.644610  8.385413  3.815012  6.156475\n",
      "6935 2023-12-28   8.119274  5.506321  1.871165  2.485123\n",
      "6936 2023-12-29   6.897775  4.161797  0.849568  1.481953\n",
      "6937 2023-12-30   9.153935  5.501731  1.626224  2.064372\n",
      "6938 2023-12-31   6.586321  6.814672  4.236740  3.916140\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all CSV files in the directory\n",
    "for file in os.listdir(csv_directory_NO2):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NO2, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Transform the 'DatoMaerke' into a column of dates and a column of times\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['DatoMaerke']).dt.date\n",
    "            df['Time'] = pd.to_datetime(df['DatoMaerke']).dt.time\n",
    "            df.drop(columns=['DatoMaerke'], inplace=True)\n",
    "        # Drop the time column if it exists\n",
    "        if 'Time' in df.columns:\n",
    "            df.drop(columns=['Time'], inplace=True)\n",
    "        # Group the data by date and calculate the mean NO2 values\n",
    "        df = df.groupby('Date').mean()\n",
    "\n",
    "        # Reindex to match the combined_df date range\n",
    "        df = df.reindex(date_range)\n",
    "        # Add the NO2 values as a new column in combined_df\n",
    "        column_name = os.path.splitext(file)[0]  # Use the file name (without extension) as the column name\n",
    "        combined_df[column_name] = df['NO2 ppb'].values\n",
    "\n",
    "# Rename the columns to 'NO2_HCAB', 'NO2_JAGT', 'NO2_HCØ' and 'NO2_HVID'\n",
    "combined_df = combined_df.rename(columns={combined_df.columns[1]: 'NO2_HCAB',\n",
    "                       combined_df.columns[2]: 'NO2_JAGT',\n",
    "                       combined_df.columns[3]: 'NO2_HCØ',\n",
    "                       combined_df.columns[4]: 'NO2_HVID'})\n",
    "\n",
    "# Print the first few rows of the updated combined DataFrame\n",
    "print(\"Updated Combined DataFrame with NO2 Values:\")\n",
    "print(combined_df.head())\n",
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87f1a2",
   "metadata": {},
   "source": [
    "The NOx should also be added in the same way as the NO2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf5afd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Combined DataFrame with NOx Values:\n",
      "        Date  NO2_HCAB  NO2_JAGT  NO2_HCØ  NO2_HVID   NOx_HCAB   NOx_JAGT  \\\n",
      "0 2005-01-01       NaN       NaN      NaN       NaN  43.682987  38.077831   \n",
      "1 2005-01-02       NaN       NaN      NaN       NaN  16.190796  12.225522   \n",
      "2 2005-01-03       NaN       NaN      NaN       NaN  34.915861  15.252096   \n",
      "3 2005-01-04       NaN       NaN      NaN       NaN  32.540399  18.183066   \n",
      "4 2005-01-05       NaN       NaN      NaN       NaN  55.714600  33.479079   \n",
      "\n",
      "     NOx_HCØ  NOx_HVID  \n",
      "0  10.268396       NaN  \n",
      "1   5.092584       NaN  \n",
      "2   8.584316       NaN  \n",
      "3   8.705654       NaN  \n",
      "4  12.964224       NaN  \n",
      "           Date   NO2_HCAB  NO2_JAGT   NO2_HCØ  NO2_HVID   NOx_HCAB  \\\n",
      "6934 2023-12-27  11.644610  8.385413  3.815012  6.156475  21.084549   \n",
      "6935 2023-12-28   8.119274  5.506321  1.871165  2.485123  12.921235   \n",
      "6936 2023-12-29   6.897775  4.161797  0.849568  1.481953  11.556676   \n",
      "6937 2023-12-30   9.153935  5.501731  1.626224  2.064372  16.678878   \n",
      "6938 2023-12-31   6.586321  6.814672  4.236740  3.916140  10.198280   \n",
      "\n",
      "       NOx_JAGT   NOx_HCØ  NOx_HVID  \n",
      "6934  13.422952  3.983214  7.862207  \n",
      "6935   8.633315  1.907765  2.526577  \n",
      "6936   6.345956  0.863469  1.543493  \n",
      "6937   8.388270  1.700224  2.251147  \n",
      "6938  10.603858  4.510658  4.223581  \n"
     ]
    }
   ],
   "source": [
    "csv_directory_NOx = './Air Quality dataset/NOx' \n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for file in os.listdir(csv_directory_NOx):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NOx, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Transform the 'DatoMaerke' into a column of dates and a column of times\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['DatoMaerke']).dt.date\n",
    "            df['Time'] = pd.to_datetime(df['DatoMaerke']).dt.time\n",
    "            df.drop(columns=['DatoMaerke'], inplace=True)\n",
    "        # Drop the time column if it exists\n",
    "        if 'Time' in df.columns:\n",
    "            df.drop(columns=['Time'], inplace=True)\n",
    "        # Group the data by date and calculate the mean NO2 values\n",
    "        df = df.groupby('Date').mean()\n",
    "\n",
    "        # Reindex to match the combined_df date range\n",
    "        df = df.reindex(date_range)\n",
    "        # Add the NO2 values as a new column in combined_df\n",
    "        column_name = os.path.splitext(file)[0]  # Use the file name (without extension) as the column name\n",
    "        combined_df[column_name] = df['NOx ppb'].values\n",
    "\n",
    "# Rename the columns appropriately for NOx data\n",
    "combined_df = combined_df.rename(columns={combined_df.columns[5]: 'NOx_HCAB',\n",
    "                       combined_df.columns[6]: 'NOx_JAGT',\n",
    "                       combined_df.columns[7]: 'NOx_HCØ',\n",
    "                       combined_df.columns[8]: 'NOx_HVID'})\n",
    "\n",
    "# Print the first few rows of the updated combined DataFrame\n",
    "print(\"Updated Combined DataFrame with NOx Values:\")\n",
    "print(combined_df.head())\n",
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36022e",
   "metadata": {},
   "source": [
    "### Adding a weekday column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799b85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame with Weekday Column:\n",
      "        Date    Weekday  NO2_HCAB  NO2_JAGT  NO2_HCØ  NO2_HVID   NOx_HCAB  \\\n",
      "0 2005-01-01   Saturday       NaN       NaN      NaN       NaN  43.682987   \n",
      "1 2005-01-02     Sunday       NaN       NaN      NaN       NaN  16.190796   \n",
      "2 2005-01-03     Monday       NaN       NaN      NaN       NaN  34.915861   \n",
      "3 2005-01-04    Tuesday       NaN       NaN      NaN       NaN  32.540399   \n",
      "4 2005-01-05  Wednesday       NaN       NaN      NaN       NaN  55.714600   \n",
      "\n",
      "    NOx_JAGT    NOx_HCØ  NOx_HVID  \n",
      "0  38.077831  10.268396       NaN  \n",
      "1  12.225522   5.092584       NaN  \n",
      "2  15.252096   8.584316       NaN  \n",
      "3  18.183066   8.705654       NaN  \n",
      "4  33.479079  12.964224       NaN  \n"
     ]
    }
   ],
   "source": [
    "# Add a new column for the weekday corresponding to the 'Date' column\n",
    "combined_df['Weekday'] = combined_df['Date'].dt.day_name()\n",
    "\n",
    "# Reorder columns to make 'Weekday' the second column\n",
    "columns = list(combined_df.columns)\n",
    "columns.insert(1, columns.pop(columns.index('Weekday')))\n",
    "combined_df = combined_df[columns]\n",
    "\n",
    "# Print the first few rows to verify the new column\n",
    "print(\"Combined DataFrame with Weekday Column:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94df56",
   "metadata": {},
   "source": [
    "### Saving the data\n",
    "Now, the new data should be saved as a new csv-file called combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b79b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./Air Quality dataset/combined_air_full_wide.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the combined DataFrame to a CSV file and a Excel file (Modeule: oipenpyxl needs to be installed with pip)\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "combined_df.to_excel(output_file.replace('.csv', '.xlsx'), index=False)\n",
    "print(f\"Data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9124d73",
   "metadata": {},
   "source": [
    "## Hvidovre dataset (hourly)\n",
    "The below code is hourly data, and is imported in the same fashion as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec98148",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Combined DataFrame with NO2 and NOx Values for 2020-2023:\n",
      "            DatoMaerke    Weekday        Date      Hour   NO2_ppb   NOx_ppb\n",
      "0  2020-01-01 00:00:00  Wednesday  2020-01-01  00:00:00  8.287385  9.406047\n",
      "1  2020-01-01 01:00:00  Wednesday  2020-01-01  01:00:00  5.106675  5.388229\n",
      "2  2020-01-01 02:00:00  Wednesday  2020-01-01  02:00:00  4.295781  4.188985\n",
      "3  2020-01-01 03:00:00  Wednesday  2020-01-01  03:00:00  2.649493  2.579914\n",
      "4  2020-01-01 04:00:00  Wednesday  2020-01-01  04:00:00  4.290077  3.901728\n",
      "                DatoMaerke Weekday        Date      Hour   NO2_ppb   NOx_ppb\n",
      "35059  2023-12-31 19:00:00  Sunday  2023-12-31  19:00:00  2.432005  2.872579\n",
      "35060  2023-12-31 20:00:00  Sunday  2023-12-31  20:00:00  2.298992  2.836902\n",
      "35061  2023-12-31 21:00:00  Sunday  2023-12-31  21:00:00  2.288536  2.677880\n",
      "35062  2023-12-31 22:00:00  Sunday  2023-12-31  22:00:00  2.320657  2.407748\n",
      "35063  2023-12-31 23:00:00  Sunday  2023-12-31  23:00:00  3.197052  3.335372\n"
     ]
    }
   ],
   "source": [
    "# Process the NO2 file\n",
    "file_no2 = 'NO2_773_HVID_2020-2023.csv'\n",
    "file_path_no2 = os.path.join(csv_directory_NO2, file_no2)\n",
    "\n",
    "# Initialize an empty DataFrame for combining data\n",
    "combined_df_hourly = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(file_path_no2):\n",
    "    df_hvid = pd.read_csv(file_path_no2)\n",
    "\n",
    "    # Adding a weekday column\n",
    "    df_hvid['Weekday'] = pd.to_datetime(df_hvid['DatoMaerke']).dt.day_name()\n",
    "\n",
    "    # Retain 'DatoMaerke' and create 'Date' and 'Hour' columns\n",
    "    if 'DatoMaerke' in df_hvid.columns:\n",
    "        df_hvid['Date'] = pd.to_datetime(df_hvid['DatoMaerke']).dt.date\n",
    "        df_hvid['Hour'] = pd.to_datetime(df_hvid['DatoMaerke']).dt.time\n",
    "\n",
    "    combined_df_hourly = df_hvid.copy()\n",
    "\n",
    "    # Reorganize the 'NO2 ppb' column to be the last column and rename it to NO2_ppb\n",
    "    if 'NO2 ppb' in combined_df_hourly.columns:\n",
    "        combined_df_hourly.rename(columns={'NO2 ppb': 'NO2_ppb'}, inplace=True)\n",
    "        # Move the 'NO2_ppb' column to the end\n",
    "        cols = list(combined_df_hourly.columns)\n",
    "        cols.append(cols.pop(cols.index('NO2_ppb')))\n",
    "        combined_df_hourly = combined_df_hourly[cols]\n",
    "\n",
    "# Process the NOx file\n",
    "file_nox = 'NOx_773_HVID_2020-2023.csv'\n",
    "file_path_nox = os.path.join(csv_directory_NOx, file_nox)\n",
    "\n",
    "if os.path.exists(file_path_nox):\n",
    "    df_nox = pd.read_csv(file_path_nox)\n",
    "    \n",
    "    # Retain 'DatoMaerke' and create 'Date' and 'Hour' columns\n",
    "    if 'DatoMaerke' in df_nox.columns:\n",
    "        df_nox['Date'] = pd.to_datetime(df_nox['DatoMaerke']).dt.date\n",
    "        df_nox['Hour'] = pd.to_datetime(df_nox['DatoMaerke']).dt.time\n",
    "\n",
    "    # Add the NOx values as a new column in combined_df\n",
    "    combined_df_hourly['NOx_ppb'] = df_nox['NOx ppb'].values\n",
    "\n",
    "\n",
    "# Print the first few rows of the updated combined DataFrame\n",
    "print(\"Updated Combined DataFrame with NO2 and NOx Values for 2020-2023:\")\n",
    "print(combined_df_hourly.head())\n",
    "print(combined_df_hourly.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dca5ad",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ac7487",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./Air Quality dataset/hourly_air_data_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the data as a CSV file\n",
    "output_file = './Air Quality dataset/hourly_air_data_full.csv'  # Output file path\n",
    "combined_df_hourly.to_csv(output_file, index=False)\n",
    "combined_df_hourly.to_excel(output_file.replace('.csv', '.xlsx'), index=False)\n",
    "print(f\"Data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa72a98",
   "metadata": {},
   "source": [
    "## Weather data preparation\n",
    "\n",
    "This is concerning the weather data and aggregating everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a025b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Weather Data:\n",
      "         Date  Hour            observed  temp_dry   Humidity  Wind_speed  \\\n",
      "0  2020-01-01     0 2020-01-01 00:25:00  1.866667  91.333333    3.516667   \n",
      "1  2020-01-01     1 2020-01-01 01:25:00  1.433333  93.333333    3.600000   \n",
      "2  2020-01-01     2 2020-01-01 02:25:00  2.083333  92.166667    3.350000   \n",
      "3  2020-01-01     3 2020-01-01 03:25:00  2.283333  92.166667    3.433333   \n",
      "4  2020-01-01     4 2020-01-01 04:25:00  2.716667  90.333333    3.516667   \n",
      "\n",
      "      Pressure    Visibility  Wind_dir  \n",
      "0  1029.250000   3350.000000     284.0  \n",
      "1  1029.083333  10500.000000     252.0  \n",
      "2  1029.150000  18333.333333     248.0  \n",
      "3  1028.966667  23833.333333     270.0  \n",
      "4  1028.783333  30333.333333     252.0  \n"
     ]
    }
   ],
   "source": [
    "# Path to the Excel file\n",
    "weather_file_path = './Weather data/Weather dataset 2020.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "weather_df = pd.read_excel(weather_file_path)\n",
    "\n",
    "weather_df['Date'] = weather_df['observed'].dt.date\n",
    "weather_df['Hour'] = weather_df['observed'].dt.hour\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and select the first value in the 'Wind_dir' column\n",
    "wind_dir_df = weather_df.groupby(['Date', 'Hour'])['Wind_dir'].first().reset_index()\n",
    "# Delete the 'Wind_dir' column from the weather_df DataFrame\n",
    "weather_df.drop(columns=['Wind_dir'], inplace=True)\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and calculate the mean for each group\n",
    "weather_df = weather_df.groupby(['Date', 'Hour']).mean().reset_index()\n",
    "\n",
    "# append the wind_dir_df to the weather_df DataFrame\n",
    "weather_df = pd.merge(weather_df, wind_dir_df, on=['Date', 'Hour'], how='left')\n",
    "\n",
    "# Print the first few rows to verify the grouped data\n",
    "print(\"Grouped Weather Data:\")\n",
    "print(weather_df.head())\n",
    "\n",
    "# Save it as a CSV file and an excel file\n",
    "weather_df.to_csv('./Weather data/weather_data_hourly_2020.csv', index=False)\n",
    "weather_df.to_excel('./Weather data/weather_data_hourly_2020.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78ae8f",
   "metadata": {},
   "source": [
    "And for 2023..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b31efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Weather Data:\n",
      "         Date  Hour            observed   Temp_dry   Humidity  wind_speed  \\\n",
      "0  2023-01-01     0 2023-01-01 00:25:00   8.083333  98.500000    5.600000   \n",
      "1  2023-01-01     1 2023-01-01 01:25:00   9.516667  97.500000    6.600000   \n",
      "2  2023-01-01     2 2023-01-01 02:25:00  10.200000  95.000000   10.633333   \n",
      "3  2023-01-01     3 2023-01-01 03:25:00  10.033333  95.666667   10.300000   \n",
      "4  2023-01-01     4 2023-01-01 04:25:00  10.166667  95.000000   10.300000   \n",
      "\n",
      "      Pressure   visibility  Wind_dir  \n",
      "0  1000.050000  3600.000000     189.0  \n",
      "1   999.616667  4250.000000     188.0  \n",
      "2   999.666667  4550.000000     223.0  \n",
      "3   999.866667  3566.666667     228.0  \n",
      "4  1000.300000  3650.000000     232.0  \n"
     ]
    }
   ],
   "source": [
    "# Path to the Excel file\n",
    "weather_file_path = './Weather data/Weather dataset 2023.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "weather_df = pd.read_excel(weather_file_path)\n",
    "\n",
    "weather_df['Date'] = weather_df['observed'].dt.date\n",
    "weather_df['Hour'] = weather_df['observed'].dt.hour\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and select the first value in the 'Wind_dir' column\n",
    "wind_dir_df = weather_df.groupby(['Date', 'Hour'])['Wind_dir'].first().reset_index()\n",
    "# Delete the 'Wind_dir' column from the weather_df DataFrame\n",
    "weather_df.drop(columns=['Wind_dir'], inplace=True)\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and calculate the mean for each group\n",
    "weather_df = weather_df.groupby(['Date', 'Hour']).mean().reset_index()\n",
    "\n",
    "# append the wind_dir_df to the weather_df DataFrame\n",
    "weather_df = pd.merge(weather_df, wind_dir_df, on=['Date', 'Hour'], how='left')\n",
    "\n",
    "# Print the first few rows to verify the grouped data\n",
    "print(\"Grouped Weather Data:\")\n",
    "print(weather_df.head())\n",
    "\n",
    "# Save it as a CSV file and an excel file\n",
    "weather_df.to_csv('./Weather data/weather_data_hourly_2023.csv', index=False)\n",
    "weather_df.to_excel('./Weather data/weather_data_hourly_2023.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa2060",
   "metadata": {},
   "source": [
    "## Trafic data\n",
    "\n",
    "The traffic data should already have a reasonalbe format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27185d8b",
   "metadata": {},
   "source": [
    "## Combining all datasets\n",
    "\n",
    "The following data should combine all the datasets into a single dataframe in the /Combined data folder.\n",
    "\n",
    "### Hourly datasets\n",
    "\n",
    "Combining Hvidover dataset with the weather and traffic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10050535",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Traffic data/traffic2020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m weather_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(weather_data_path)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load the traffic dataset\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m traffic_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Traffic data/traffic2020.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Rename the column names: 'Dato' and 'Tid' to 'Date' and 'Hour'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m traffic_data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDato\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Traffic data/traffic2020.csv'"
     ]
    }
   ],
   "source": [
    "# Load the air quality dataset\n",
    "hourly_air_data_path = './Air Quality dataset/hourly_air_data_full.csv'\n",
    "output_file = './Combined data/combined_data_hourly_2020.csv'\n",
    "hourly_air_data = pd.read_csv(hourly_air_data_path)\n",
    "\n",
    "# Load the weather dataset\n",
    "weather_data_path = './Weather data/weather_data_hourly_2020.csv'\n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "\n",
    "# Load the traffic dataset\n",
    "traffic_data = pd.read_csv('./Traffic data/traffic2020.csv')\n",
    "# Rename the column names: 'Dato' and 'Tid' to 'Date' and 'Hour'\n",
    "traffic_data.rename(columns={'Dato': 'Date', 'Tid': 'Hour'}, inplace=True)\n",
    "\n",
    "# Change the column 'Hour' to only include the starting hour of the air data and traffic data\n",
    "hourly_air_data['Hour'] = pd.to_datetime(hourly_air_data['Hour']).dt.hour\n",
    "traffic_data['Hour'] = pd.to_datetime(traffic_data['Hour']).dt.hour\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format for both datasets\n",
    "hourly_air_data['Date'] = pd.to_datetime(hourly_air_data['Date'])\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "traffic_data['Date'] = pd.to_datetime(traffic_data['Date'])\n",
    "\n",
    "# Merge the datasets on 'Date' and 'Hour'\n",
    "merged_data = pd.merge(hourly_air_data, weather_data, on=['Date', 'Hour'], how='inner')\n",
    "\n",
    "# Remove unnecessary columns from the merged_data dataframe\n",
    "merged_data.drop(columns=['DatoMaerke'], inplace=True)\n",
    "\n",
    "# Print the first few rows of the merged dataset\n",
    "print(\"Merged Dataset:\")\n",
    "print(merged_data.head())\n",
    "print(traffic_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e06748",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file as csv and excel\n",
    "merged_data.to_csv(output_file, index=False)\n",
    "merged_data.to_excel(output_file.replace('.csv', '.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
