{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c66e505",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Preparation of multiple datasets\n",
    "## Air quality (Daily)\n",
    "\n",
    "The full dataset is only daily data, and is aggreagting data where multiple time of the day exists by the mean. \n",
    "\n",
    "Importing the notebooks for preparing the data. The data preparation should combine different csv files into a single combined csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bf4f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "csv_directory_NO2 = './Air Quality dataset/NO2'  # Update this path to your folder containing CSV files\n",
    "csv_directory_NOx = './Air Quality dataset/NOx'  # Update this path to your folder containing CSV files\n",
    "output_file = './Air Quality dataset/combined_air_full_wide.csv'  # Output file path\n",
    "\n",
    "# Initialize an empty DataFrame for combining data\n",
    "combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f913b",
   "metadata": {},
   "source": [
    "The following code should create a date variable, that stretches from the earliest recorded date to the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf6d475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame with Date Range:\n",
      "        Date\n",
      "0 2005-01-01\n",
      "1 2005-01-02\n",
      "2 2005-01-03\n",
      "3 2005-01-04\n",
      "4 2005-01-05\n",
      "           Date\n",
      "6934 2023-12-27\n",
      "6935 2023-12-28\n",
      "6936 2023-12-29\n",
      "6937 2023-12-30\n",
      "6938 2023-12-31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List to store all dates\n",
    "all_dates = []\n",
    "\n",
    "# Iterate through all CSV files in both directories\n",
    "for file in os.listdir(csv_directory_NO2):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NO2, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            all_dates.extend(pd.to_datetime(df['DatoMaerke']).tolist())\n",
    "for file in os.listdir(csv_directory_NOx):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NOx, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            all_dates.extend(pd.to_datetime(df['DatoMaerke']).tolist())\n",
    "\n",
    "# Create a date range from the earliest to the latest date\n",
    "if all_dates:\n",
    "    min_date = min(all_dates)\n",
    "    max_date = max(all_dates)\n",
    "    date_range = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "    # Add the date range to the combined_df\n",
    "    combined_df['Date'] = date_range\n",
    "\n",
    "# Print the first few rows of the combined DataFrame\n",
    "print(\"Combined DataFrame with Date Range:\")\n",
    "print(combined_df.head())\n",
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58a00d",
   "metadata": {},
   "source": [
    "Now, the different NO2 values should be added. The NO2 values are stored in the column called 'NO2 ppb', and each new column should have the name of the file that it was extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0156b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Combined DataFrame with NO2 Values:\n",
      "        Date  NO2_HCAB  NO2_JAGT  NO2_HCØ  NO2_HVID\n",
      "0 2005-01-01       NaN       NaN      NaN       NaN\n",
      "1 2005-01-02       NaN       NaN      NaN       NaN\n",
      "2 2005-01-03       NaN       NaN      NaN       NaN\n",
      "3 2005-01-04       NaN       NaN      NaN       NaN\n",
      "4 2005-01-05       NaN       NaN      NaN       NaN\n",
      "           Date   NO2_HCAB  NO2_JAGT   NO2_HCØ  NO2_HVID\n",
      "6934 2023-12-27  11.644610  8.385413  3.815012  6.156475\n",
      "6935 2023-12-28   8.119274  5.506321  1.871165  2.485123\n",
      "6936 2023-12-29   6.897775  4.161797  0.849568  1.481953\n",
      "6937 2023-12-30   9.153935  5.501731  1.626224  2.064372\n",
      "6938 2023-12-31   6.586321  6.814672  4.236740  3.916140\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all CSV files in the directory\n",
    "for file in os.listdir(csv_directory_NO2):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NO2, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Transform the 'DatoMaerke' into a column of dates and a column of times\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['DatoMaerke']).dt.date\n",
    "            df['Time'] = pd.to_datetime(df['DatoMaerke']).dt.time\n",
    "            df.drop(columns=['DatoMaerke'], inplace=True)\n",
    "        # Drop the time column if it exists\n",
    "        if 'Time' in df.columns:\n",
    "            df.drop(columns=['Time'], inplace=True)\n",
    "        # Group the data by date and calculate the mean NO2 values\n",
    "        df = df.groupby('Date').mean()\n",
    "\n",
    "        # Reindex to match the combined_df date range\n",
    "        df = df.reindex(date_range)\n",
    "        # Add the NO2 values as a new column in combined_df\n",
    "        column_name = os.path.splitext(file)[0]  # Use the file name (without extension) as the column name\n",
    "        combined_df[column_name] = df['NO2 ppb'].values\n",
    "\n",
    "# Rename the columns to 'NO2_HCAB', 'NO2_JAGT', 'NO2_HCØ' and 'NO2_HVID'\n",
    "combined_df = combined_df.rename(columns={combined_df.columns[1]: 'NO2_HCAB',\n",
    "                       combined_df.columns[2]: 'NO2_JAGT',\n",
    "                       combined_df.columns[3]: 'NO2_HCØ',\n",
    "                       combined_df.columns[4]: 'NO2_HVID'})\n",
    "\n",
    "# Print the first few rows of the updated combined DataFrame\n",
    "print(\"Updated Combined DataFrame with NO2 Values:\")\n",
    "print(combined_df.head())\n",
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87f1a2",
   "metadata": {},
   "source": [
    "The NOx should also be added in the same way as the NO2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bf5afd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Combined DataFrame with NOx Values:\n",
      "        Date  NO2_HCAB  NO2_JAGT  NO2_HCØ  NO2_HVID   NOx_HCAB   NOx_JAGT  \\\n",
      "0 2005-01-01       NaN       NaN      NaN       NaN  43.682987  38.077831   \n",
      "1 2005-01-02       NaN       NaN      NaN       NaN  16.190796  12.225522   \n",
      "2 2005-01-03       NaN       NaN      NaN       NaN  34.915861  15.252096   \n",
      "3 2005-01-04       NaN       NaN      NaN       NaN  32.540399  18.183066   \n",
      "4 2005-01-05       NaN       NaN      NaN       NaN  55.714600  33.479079   \n",
      "\n",
      "     NOx_HCØ  NOx_HVID  \n",
      "0  10.268396       NaN  \n",
      "1   5.092584       NaN  \n",
      "2   8.584316       NaN  \n",
      "3   8.705654       NaN  \n",
      "4  12.964224       NaN  \n",
      "           Date   NO2_HCAB  NO2_JAGT   NO2_HCØ  NO2_HVID   NOx_HCAB  \\\n",
      "6934 2023-12-27  11.644610  8.385413  3.815012  6.156475  21.084549   \n",
      "6935 2023-12-28   8.119274  5.506321  1.871165  2.485123  12.921235   \n",
      "6936 2023-12-29   6.897775  4.161797  0.849568  1.481953  11.556676   \n",
      "6937 2023-12-30   9.153935  5.501731  1.626224  2.064372  16.678878   \n",
      "6938 2023-12-31   6.586321  6.814672  4.236740  3.916140  10.198280   \n",
      "\n",
      "       NOx_JAGT   NOx_HCØ  NOx_HVID  \n",
      "6934  13.422952  3.983214  7.862207  \n",
      "6935   8.633315  1.907765  2.526577  \n",
      "6936   6.345956  0.863469  1.543493  \n",
      "6937   8.388270  1.700224  2.251147  \n",
      "6938  10.603858  4.510658  4.223581  \n"
     ]
    }
   ],
   "source": [
    "csv_directory_NOx = './Air Quality dataset/NOx' \n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for file in os.listdir(csv_directory_NOx):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory_NOx, file)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Transform the 'DatoMaerke' into a column of dates and a column of times\n",
    "        if 'DatoMaerke' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['DatoMaerke']).dt.date\n",
    "            df['Time'] = pd.to_datetime(df['DatoMaerke']).dt.time\n",
    "            df.drop(columns=['DatoMaerke'], inplace=True)\n",
    "        # Drop the time column if it exists\n",
    "        if 'Time' in df.columns:\n",
    "            df.drop(columns=['Time'], inplace=True)\n",
    "        # Group the data by date and calculate the mean NO2 values\n",
    "        df = df.groupby('Date').mean()\n",
    "\n",
    "        # Reindex to match the combined_df date range\n",
    "        df = df.reindex(date_range)\n",
    "        # Add the NO2 values as a new column in combined_df\n",
    "        column_name = os.path.splitext(file)[0]  # Use the file name (without extension) as the column name\n",
    "        combined_df[column_name] = df['NOx ppb'].values\n",
    "\n",
    "# Rename the columns appropriately for NOx data\n",
    "combined_df = combined_df.rename(columns={combined_df.columns[5]: 'NOx_HCAB',\n",
    "                       combined_df.columns[6]: 'NOx_JAGT',\n",
    "                       combined_df.columns[7]: 'NOx_HCØ',\n",
    "                       combined_df.columns[8]: 'NOx_HVID'})\n",
    "\n",
    "# Print the first few rows of the updated combined DataFrame\n",
    "print(\"Updated Combined DataFrame with NOx Values:\")\n",
    "print(combined_df.head())\n",
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36022e",
   "metadata": {},
   "source": [
    "### Adding a weekday column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "799b85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame with Weekday Column:\n",
      "        Date    Weekday  NO2_HCAB  NO2_JAGT  NO2_HCØ  NO2_HVID   NOx_HCAB  \\\n",
      "0 2005-01-01   Saturday       NaN       NaN      NaN       NaN  43.682987   \n",
      "1 2005-01-02     Sunday       NaN       NaN      NaN       NaN  16.190796   \n",
      "2 2005-01-03     Monday       NaN       NaN      NaN       NaN  34.915861   \n",
      "3 2005-01-04    Tuesday       NaN       NaN      NaN       NaN  32.540399   \n",
      "4 2005-01-05  Wednesday       NaN       NaN      NaN       NaN  55.714600   \n",
      "\n",
      "    NOx_JAGT    NOx_HCØ  NOx_HVID  \n",
      "0  38.077831  10.268396       NaN  \n",
      "1  12.225522   5.092584       NaN  \n",
      "2  15.252096   8.584316       NaN  \n",
      "3  18.183066   8.705654       NaN  \n",
      "4  33.479079  12.964224       NaN  \n"
     ]
    }
   ],
   "source": [
    "# Add a new column for the weekday corresponding to the 'Date' column\n",
    "combined_df['Weekday'] = combined_df['Date'].dt.day_name()\n",
    "\n",
    "# Reorder columns to make 'Weekday' the second column\n",
    "columns = list(combined_df.columns)\n",
    "columns.insert(1, columns.pop(columns.index('Weekday')))\n",
    "combined_df = combined_df[columns]\n",
    "\n",
    "# Print the first few rows to verify the new column\n",
    "print(\"Combined DataFrame with Weekday Column:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94df56",
   "metadata": {},
   "source": [
    "### Saving the data\n",
    "Now, the new data should be saved as a new csv-file called combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b79b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./Air Quality dataset/combined_air_full_wide.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the combined DataFrame to a CSV file and a Excel file (Modeule: oipenpyxl needs to be installed with pip)\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "combined_df.to_excel(output_file.replace('.csv', '.xlsx'), index=False)\n",
    "print(f\"Data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9124d73",
   "metadata": {},
   "source": [
    "## Hvidovre dataset (hourly)\n",
    "The below code is hourly data, and is imported in the same fashion as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eec98148",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Combined DataFrame with NO2 and NOx Values for 2020-2023:\n",
      "            DatoMaerke    Weekday        Date      Hour   NO2_ppb   NOx_ppb\n",
      "0  2020-01-01 00:00:00  Wednesday  2020-01-01  00:00:00  8.287385  9.406047\n",
      "1  2020-01-01 01:00:00  Wednesday  2020-01-01  01:00:00  5.106675  5.388229\n",
      "2  2020-01-01 02:00:00  Wednesday  2020-01-01  02:00:00  4.295781  4.188985\n",
      "3  2020-01-01 03:00:00  Wednesday  2020-01-01  03:00:00  2.649493  2.579914\n",
      "4  2020-01-01 04:00:00  Wednesday  2020-01-01  04:00:00  4.290077  3.901728\n",
      "                DatoMaerke Weekday        Date      Hour   NO2_ppb   NOx_ppb\n",
      "35059  2023-12-31 19:00:00  Sunday  2023-12-31  19:00:00  2.432005  2.872579\n",
      "35060  2023-12-31 20:00:00  Sunday  2023-12-31  20:00:00  2.298992  2.836902\n",
      "35061  2023-12-31 21:00:00  Sunday  2023-12-31  21:00:00  2.288536  2.677880\n",
      "35062  2023-12-31 22:00:00  Sunday  2023-12-31  22:00:00  2.320657  2.407748\n",
      "35063  2023-12-31 23:00:00  Sunday  2023-12-31  23:00:00  3.197052  3.335372\n"
     ]
    }
   ],
   "source": [
    "# Process the NO2 file\n",
    "file_no2 = 'NO2_773_HVID_2020-2023.csv'\n",
    "file_path_no2 = os.path.join(csv_directory_NO2, file_no2)\n",
    "\n",
    "# Initialize an empty DataFrame for combining data\n",
    "combined_df_hourly = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(file_path_no2):\n",
    "    df_hvid = pd.read_csv(file_path_no2)\n",
    "\n",
    "    # Adding a weekday column\n",
    "    df_hvid['Weekday'] = pd.to_datetime(df_hvid['DatoMaerke']).dt.day_name()\n",
    "\n",
    "    # Retain 'DatoMaerke' and create 'Date' and 'Hour' columns\n",
    "    if 'DatoMaerke' in df_hvid.columns:\n",
    "        df_hvid['Date'] = pd.to_datetime(df_hvid['DatoMaerke']).dt.date\n",
    "        df_hvid['Hour'] = pd.to_datetime(df_hvid['DatoMaerke']).dt.time\n",
    "\n",
    "    combined_df_hourly = df_hvid.copy()\n",
    "\n",
    "    # Reorganize the 'NO2 ppb' column to be the last column and rename it to NO2_ppb\n",
    "    if 'NO2 ppb' in combined_df_hourly.columns:\n",
    "        combined_df_hourly.rename(columns={'NO2 ppb': 'NO2_ppb'}, inplace=True)\n",
    "        # Move the 'NO2_ppb' column to the end\n",
    "        cols = list(combined_df_hourly.columns)\n",
    "        cols.append(cols.pop(cols.index('NO2_ppb')))\n",
    "        combined_df_hourly = combined_df_hourly[cols]\n",
    "\n",
    "# Process the NOx file\n",
    "file_nox = 'NOx_773_HVID_2020-2023.csv'\n",
    "file_path_nox = os.path.join(csv_directory_NOx, file_nox)\n",
    "\n",
    "if os.path.exists(file_path_nox):\n",
    "    df_nox = pd.read_csv(file_path_nox)\n",
    "    \n",
    "    # Retain 'DatoMaerke' and create 'Date' and 'Hour' columns\n",
    "    if 'DatoMaerke' in df_nox.columns:\n",
    "        df_nox['Date'] = pd.to_datetime(df_nox['DatoMaerke']).dt.date\n",
    "        df_nox['Hour'] = pd.to_datetime(df_nox['DatoMaerke']).dt.time\n",
    "\n",
    "    # Add the NOx values as a new column in combined_df\n",
    "    combined_df_hourly['NOx_ppb'] = df_nox['NOx ppb'].values\n",
    "\n",
    "\n",
    "# Print the first few rows of the updated combined DataFrame\n",
    "print(\"Updated Combined DataFrame with NO2 and NOx Values for 2020-2023:\")\n",
    "print(combined_df_hourly.head())\n",
    "print(combined_df_hourly.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dca5ad",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7ac7487",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./Air Quality dataset/hourly_air_data_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the data as a CSV file\n",
    "output_file = './Air Quality dataset/hourly_air_data_full.csv'  # Output file path\n",
    "combined_df_hourly.to_csv(output_file, index=False)\n",
    "combined_df_hourly.to_excel(output_file.replace('.csv', '.xlsx'), index=False)\n",
    "print(f\"Data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa72a98",
   "metadata": {},
   "source": [
    "## Weather data preparation\n",
    "\n",
    "This is concerning the weather data and aggregating everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a025b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Weather Data:\n",
      "         Date  Hour  temp_dry   Humidity  Wind_speed     Pressure  \\\n",
      "0  2020-01-01     0  1.866667  91.333333    3.516667  1029.250000   \n",
      "1  2020-01-01     1  1.433333  93.333333    3.600000  1029.083333   \n",
      "2  2020-01-01     2  2.083333  92.166667    3.350000  1029.150000   \n",
      "3  2020-01-01     3  2.283333  92.166667    3.433333  1028.966667   \n",
      "4  2020-01-01     4  2.716667  90.333333    3.516667  1028.783333   \n",
      "\n",
      "     Visibility  Wind_dir  \n",
      "0   3350.000000     284.0  \n",
      "1  10500.000000     252.0  \n",
      "2  18333.333333     248.0  \n",
      "3  23833.333333     270.0  \n",
      "4  30333.333333     252.0  \n"
     ]
    }
   ],
   "source": [
    "# Path to the Excel file\n",
    "weather_file_path = './Weather data/Weather dataset 2020.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "weather_df = pd.read_excel(weather_file_path)\n",
    "\n",
    "weather_df['Date'] = weather_df['observed'].dt.date\n",
    "weather_df['Hour'] = weather_df['observed'].dt.hour\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and select the first value in the 'Wind_dir' column\n",
    "wind_dir_df = weather_df.groupby(['Date', 'Hour'])['Wind_dir'].first().reset_index()\n",
    "# Delete the 'Wind_dir' column from the weather_df DataFrame\n",
    "weather_df.drop(columns=['Wind_dir', 'observed'], inplace=True)\n",
    "\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and calculate the mean for each group\n",
    "weather_df = weather_df.groupby(['Date', 'Hour']).mean().reset_index()\n",
    "\n",
    "# append the wind_dir_df to the weather_df DataFrame\n",
    "weather_df = pd.merge(weather_df, wind_dir_df, on=['Date', 'Hour'], how='left')\n",
    "\n",
    "# Print the first few rows to verify the grouped data\n",
    "print(\"Grouped Weather Data:\")\n",
    "print(weather_df.head())\n",
    "\n",
    "# Save it as a CSV file and an excel file\n",
    "weather_df.to_csv('./Weather data/weather_data_hourly_2020.csv', index=False)\n",
    "weather_df.to_excel('./Weather data/weather_data_hourly_2020.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78ae8f",
   "metadata": {},
   "source": [
    "And for 2023..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60b31efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Weather Data:\n",
      "         Date  Hour   Temp_dry   Humidity  wind_speed     Pressure  \\\n",
      "0  2023-01-01     0   8.083333  98.500000    5.600000  1000.050000   \n",
      "1  2023-01-01     1   9.516667  97.500000    6.600000   999.616667   \n",
      "2  2023-01-01     2  10.200000  95.000000   10.633333   999.666667   \n",
      "3  2023-01-01     3  10.033333  95.666667   10.300000   999.866667   \n",
      "4  2023-01-01     4  10.166667  95.000000   10.300000  1000.300000   \n",
      "\n",
      "    visibility  Wind_dir  \n",
      "0  3600.000000     189.0  \n",
      "1  4250.000000     188.0  \n",
      "2  4550.000000     223.0  \n",
      "3  3566.666667     228.0  \n",
      "4  3650.000000     232.0  \n"
     ]
    }
   ],
   "source": [
    "# Path to the Excel file\n",
    "weather_file_path = './Weather data/Weather dataset 2023.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "weather_df = pd.read_excel(weather_file_path)\n",
    "\n",
    "weather_df['Date'] = weather_df['observed'].dt.date\n",
    "weather_df['Hour'] = weather_df['observed'].dt.hour\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and select the first value in the 'Wind_dir' column\n",
    "wind_dir_df = weather_df.groupby(['Date', 'Hour'])['Wind_dir'].first().reset_index()\n",
    "# Delete the 'Wind_dir' and 'observed' column from the weather_df DataFrame\n",
    "weather_df.drop(columns=['Wind_dir', 'observed'], inplace=True)\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and calculate the mean for each group\n",
    "weather_df = weather_df.groupby(['Date', 'Hour']).mean().reset_index()\n",
    "\n",
    "# append the wind_dir_df to the weather_df DataFrame\n",
    "weather_df = pd.merge(weather_df, wind_dir_df, on=['Date', 'Hour'], how='left')\n",
    "\n",
    "# Print the first few rows to verify the grouped data\n",
    "print(\"Grouped Weather Data:\")\n",
    "print(weather_df.head())\n",
    "\n",
    "# Save it as a CSV file and an excel file\n",
    "weather_df.to_csv('./Weather data/weather_data_hourly_2023.csv', index=False)\n",
    "weather_df.to_excel('./Weather data/weather_data_hourly_2023.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d441e",
   "metadata": {},
   "source": [
    "## and for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c231078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Weather Data:\n",
      "         Date  Hour  temp_dry   humidity  wind_speed     Pressure  \\\n",
      "0  2025-05-01     0  8.566667  89.833333    1.716667  1021.850000   \n",
      "1  2025-05-01     1  7.050000  94.333333    1.416667  1021.700000   \n",
      "2  2025-05-01     2  6.883333  94.333333    1.083333  1021.700000   \n",
      "3  2025-05-01     3  7.566667  97.833333    1.016667  1021.700000   \n",
      "4  2025-05-01     4  7.133333  95.333333    0.333333  1021.833333   \n",
      "\n",
      "     Visibility  Wind_dir  \n",
      "0  32833.333333       340  \n",
      "1  14833.333333       250  \n",
      "2   7950.000000       256  \n",
      "3   8033.333333       267  \n",
      "4  10216.666667        40  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "weather_file_path = './Weather data/Weather dataset (May 2025).xlsx'\n",
    "\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "weather_df = pd.read_excel(weather_file_path)\n",
    "\n",
    "weather_df['Date'] = weather_df['observed'].dt.date\n",
    "weather_df['Hour'] = weather_df['observed'].dt.hour\n",
    "\n",
    "weather_df.rename(columns={'wind_dir': 'Wind_dir'}, inplace=True)\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and select the first value in the 'Wind_dir' column\n",
    "wind_dir_df = weather_df.groupby(['Date', 'Hour'])['Wind_dir'].first().reset_index()\n",
    "# Delete the 'Wind_dir' and 'observed' column from the weather_df DataFrame\n",
    "weather_df.drop(columns=['Wind_dir', 'observed'], inplace=True)\n",
    "\n",
    "# Group the weather data by 'Date' and 'Hour' and calculate the mean for each group\n",
    "weather_df = weather_df.groupby(['Date', 'Hour']).mean().reset_index()\n",
    "\n",
    "# append the wind_dir_df to the weather_df DataFrame\n",
    "weather_df = pd.merge(weather_df, wind_dir_df, on=['Date', 'Hour'], how='left')\n",
    "\n",
    "# Print the first few rows to verify the grouped data\n",
    "print(\"Grouped Weather Data:\")\n",
    "print(weather_df.head())\n",
    "\n",
    "# Save it as a CSV file and an excel file\n",
    "weather_df.to_csv('./Weather data/weather_data_hourly_2025.csv', index=False)\n",
    "weather_df.to_excel('./Weather data/weather_data_hourly_2025.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa2060",
   "metadata": {},
   "source": [
    "## Trafic data\n",
    "\n",
    "The traffic data should already have a reasonalbe format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27185d8b",
   "metadata": {},
   "source": [
    "## Combining all datasets\n",
    "\n",
    "The following data should combine all the datasets into a single dataframe in the /Combined data folder.\n",
    "\n",
    "### Hourly datasets\n",
    "\n",
    "Combining Hvidover dataset with the weather and traffic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10050535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset:\n",
      "     Weekday       Date  Hour   NO2_ppb   NOx_ppb  temp_dry   Humidity  \\\n",
      "0  Wednesday 2020-01-01     0  8.287385  9.406047  1.866667  91.333333   \n",
      "1  Wednesday 2020-01-01     1  5.106675  5.388229  1.433333  93.333333   \n",
      "2  Wednesday 2020-01-01     2  4.295781  4.188985  2.083333  92.166667   \n",
      "3  Wednesday 2020-01-01     3  2.649493  2.579914  2.283333  92.166667   \n",
      "4  Wednesday 2020-01-01     4  4.290077  3.901728  2.716667  90.333333   \n",
      "\n",
      "   Wind_speed     Pressure    Visibility  ...  Celle 25  Celle 26  Celle 27  \\\n",
      "0    3.516667  1029.250000   3350.000000  ...  1.695626  1.695626  1.695626   \n",
      "1    3.600000  1029.083333  10500.000000  ...  1.695626  4.239064  4.239064   \n",
      "2    3.350000  1029.150000  18333.333333  ...  1.695626  3.391251  5.934690   \n",
      "3    3.433333  1028.966667  23833.333333  ...  0.000000  0.847813  5.934690   \n",
      "4    3.516667  1028.783333  30333.333333  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "   Celle 28  Celle 29  Celle 30  Celle 31  Celle 32  Celle 33  Celle 34  \n",
      "0  3.391251  2.543438  0.000000       0.0  5.086877  4.239064  4.239064  \n",
      "1  4.239064  4.239064  1.695626       0.0  6.782503  7.630315  4.239064  \n",
      "2  5.086877  5.086877  2.543438       0.0  8.478128  5.086877  7.630315  \n",
      "3  5.086877  5.086877  0.000000       0.0  6.782503  7.630315  5.934690  \n",
      "4  2.543438  4.239064  0.000000       0.0  3.391251  5.086877  3.391251  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_14768\\3799426733.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  hourly_air_data['Hour'] = pd.to_datetime(hourly_air_data['Hour']).dt.hour\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_14768\\3799426733.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traffic_data['Hour'] = pd.to_datetime(traffic_data['Hour']).dt.hour\n"
     ]
    }
   ],
   "source": [
    "# Load the air quality dataset\n",
    "hourly_air_data_path = './Air Quality dataset/hourly_air_data_full.csv'\n",
    "hourly_air_data = pd.read_csv(hourly_air_data_path)\n",
    "\n",
    "# Load the weather dataset\n",
    "weather_data_path = './Weather data/weather_data_hourly_2020.csv'\n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "\n",
    "# Load the traffic dataset\n",
    "traffic_data = pd.read_csv('./Traffic data/traffic2020.csv', sep = \";\", decimal = \",\")\n",
    "# Rename the column names: 'Dato' and 'Tid' to 'Date' and 'Hour'\n",
    "traffic_data.rename(columns={'Dato': 'Date', 'Tid': 'Hour'}, inplace=True)\n",
    "# Transform the date column from time format 'dd-mm-yyyy' to 'mm-dd-yyyy'\n",
    "traffic_data['Date'] = pd.to_datetime(traffic_data['Date'], format='%d-%m-%Y').dt.date\n",
    "\n",
    "# Change the column 'Hour' to only include the starting hour of the air data and traffic data\n",
    "hourly_air_data['Hour'] = pd.to_datetime(hourly_air_data['Hour']).dt.hour\n",
    "traffic_data['Hour'] = pd.to_datetime(traffic_data['Hour']).dt.hour\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format for both datasets\n",
    "hourly_air_data['Date'] = pd.to_datetime(hourly_air_data['Date'])\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "traffic_data['Date'] = pd.to_datetime(traffic_data['Date'])\n",
    "\n",
    "# Merge air data and weather data  on 'Date' and 'Hour'\n",
    "merged_data = pd.merge(hourly_air_data, weather_data, on=['Date', 'Hour'], how='inner')\n",
    "# Merge the merged_data with traffic data on 'Date' and 'Hour'\n",
    "merged_data = pd.merge(merged_data, traffic_data, on=['Date', 'Hour'], how='inner')\n",
    "\n",
    "# Remove unnecessary columns from the merged_data dataframe\n",
    "merged_data.drop(columns=['DatoMaerke'], inplace=True)\n",
    "\n",
    "# Print the first few rows of the merged dataset\n",
    "print(\"Merged Dataset:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e06748",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "791d9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file as csv and excel\n",
    "output_file = './Combined data/combined_data_hourly_2020.csv'\n",
    "merged_data.to_csv(output_file, index=False)\n",
    "merged_data.to_excel(output_file.replace('.csv', '.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924b3b2",
   "metadata": {},
   "source": [
    "Do the same for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc5b895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset:\n",
      "  Weekday       Date  Hour   NO2_ppb    NOx_ppb   Temp_dry   Humidity  \\\n",
      "0  Sunday 2023-01-01     0  9.609539  14.333718   8.083333  98.500000   \n",
      "1  Sunday 2023-01-01     1  2.431203   2.601390   9.516667  97.500000   \n",
      "2  Sunday 2023-01-01     2  1.794863   1.888760  10.200000  95.000000   \n",
      "3  Sunday 2023-01-01     3  1.286549   1.169177  10.033333  95.666667   \n",
      "4  Sunday 2023-01-01     4  1.020709   0.850521  10.166667  95.000000   \n",
      "\n",
      "   wind_speed     Pressure   visibility  ...  Celle 25  Celle 26  Celle 27  \\\n",
      "0    5.600000  1000.050000  3600.000000  ...      3.73      3.73      6.54   \n",
      "1    6.600000   999.616667  4250.000000  ...      2.80      5.60      4.67   \n",
      "2   10.633333   999.666667  4550.000000  ...      1.87      4.67      4.67   \n",
      "3   10.300000   999.866667  3566.666667  ...      1.87      4.67      4.67   \n",
      "4   10.300000  1000.300000  3650.000000  ...      0.93      2.80      4.67   \n",
      "\n",
      "   Celle 28  Celle 29  Celle 30  Celle 31  Celle 32  Celle 33  Celle 34  \n",
      "0      1.87      2.80      2.80       0.0      3.73      3.73      2.80  \n",
      "1      0.93      1.87      1.87       0.0      1.87      2.80      2.80  \n",
      "2      0.93      1.87      0.93       0.0      1.87      1.87      1.87  \n",
      "3      0.93      1.87      0.00       0.0      2.80      2.80      1.87  \n",
      "4      0.93      0.93      0.00       0.0      1.87      3.73      0.93  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_14768\\885311442.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  hourly_air_data['Hour'] = pd.to_datetime(hourly_air_data['Hour']).dt.hour\n",
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_14768\\885311442.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traffic_data['Hour'] = pd.to_datetime(traffic_data['Hour']).dt.hour\n"
     ]
    }
   ],
   "source": [
    "# Load the air quality dataset\n",
    "hourly_air_data_path = './Air Quality dataset/hourly_air_data_full.csv'\n",
    "hourly_air_data = pd.read_csv(hourly_air_data_path)\n",
    "\n",
    "# Load the weather dataset\n",
    "weather_data_path = './Weather data/weather_data_hourly_2023.csv'\n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "\n",
    "# Load the traffic dataset\n",
    "traffic_data = pd.read_csv('./Traffic data/traffic2023.csv', sep = \";\", decimal = \",\")\n",
    "# Rename the column names: 'Dato' and 'Tid' to 'Date' and 'Hour'\n",
    "traffic_data.rename(columns={'Dato': 'Date', 'Tid': 'Hour'}, inplace=True)\n",
    "# Transform the date column from time format 'dd-mm-yyyy' to 'mm-dd-yyyy'\n",
    "traffic_data['Date'] = pd.to_datetime(traffic_data['Date'], format='%d-%m-%Y').dt.date\n",
    "\n",
    "# Change the column 'Hour' to only include the starting hour of the air data and traffic data\n",
    "hourly_air_data['Hour'] = pd.to_datetime(hourly_air_data['Hour']).dt.hour\n",
    "traffic_data['Hour'] = pd.to_datetime(traffic_data['Hour']).dt.hour\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format for both datasets\n",
    "hourly_air_data['Date'] = pd.to_datetime(hourly_air_data['Date'])\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "traffic_data['Date'] = pd.to_datetime(traffic_data['Date'])\n",
    "\n",
    "# Merge air data and weather data  on 'Date' and 'Hour'\n",
    "merged_data = pd.merge(hourly_air_data, weather_data, on=['Date', 'Hour'], how='inner')\n",
    "# Merge the merged_data with traffic data on 'Date' and 'Hour'\n",
    "merged_data = pd.merge(merged_data, traffic_data, on=['Date', 'Hour'], how='inner')\n",
    "\n",
    "# Remove unnecessary columns from the merged_data dataframe\n",
    "merged_data.drop(columns=['DatoMaerke'], inplace=True)\n",
    "\n",
    "# Print the first few rows of the merged dataset\n",
    "print(\"Merged Dataset:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba6400",
   "metadata": {},
   "source": [
    "Saving the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b48900c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file as csv and excel\n",
    "output_file = './Combined data/combined_data_hourly_2023.csv'\n",
    "merged_data.to_csv(output_file, index=False)\n",
    "merged_data.to_excel(output_file.replace('.csv', '.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
